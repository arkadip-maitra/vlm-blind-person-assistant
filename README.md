# 👁️‍🗨️ Qwen 2.5 VL for Blind Assistance

This repository contains code and evaluation results for a project using **Qwen 2.5 VL**, Alibaba’s open-source Vision-Language Model (VLM), aimed at assisting visually impaired individuals through intelligent scene understanding and object recognition.

## 🧠 Project Overview

The goal of this project is to explore how Qwen 2.5 VL can be used to interpret visual environments for blind users in real-time or near-real-time scenarios. The model receives visual inputs (e.g., images from a camera feed) and generates natural language descriptions to help users understand their surroundings.

This repo includes:
- 🔍 Inference scripts using Qwen 2.5 VL (Vision-Language model)
- 🧪 Evaluation results on 3 different datasets
- 📂 Model outputs in JSON format for further analysis

---
