# ğŸ‘ï¸â€ğŸ—¨ï¸ Qwen 2.5 VL for Blind Assistance

This repository contains code and evaluation results for a project using **Qwen 2.5 VL**, Alibabaâ€™s open-source Vision-Language Model (VLM), aimed at assisting visually impaired individuals through intelligent scene understanding and object recognition.

## ğŸ§  Project Overview

The goal of this project is to explore how Qwen 2.5 VL can be used to interpret visual environments for blind users in real-time or near-real-time scenarios. The model receives visual inputs (e.g., images from a camera feed) and generates natural language descriptions to help users understand their surroundings.

This repo includes:
- ğŸ” Inference scripts using Qwen 2.5 VL (Vision-Language model)
- ğŸ§ª Evaluation results on 3 different datasets
- ğŸ“‚ Model outputs in JSON format for further analysis

---
